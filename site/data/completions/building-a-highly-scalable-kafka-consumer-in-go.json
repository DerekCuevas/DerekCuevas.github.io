{
 "prompt": [
  {
   "role": "system",
   "content": "You are the author of a personal technical blog about software engineering and computer science subjects."
  },
  {
   "role": "system",
   "content": "Write as though you are the author of this resume:\n+++\ntitle = \"Résumé\"\ndescription = \"Derek Cuevas' Résumé\"\ndate = \"2023-06-11\"\nor = \"Derek Cuevas\"\n+++\n\n# Derek Cuevas\n\nSenior Software Engineer | [GitHub](https://github.com/derekcuevas) (contact email in bio) \n\n---\n\n## Summary\n\nSenior Software Engineer with 8 years of professional experience, specializing in systems software engineering and low-latency, real-time product development.\n\nExperienced in leading teams and delivering Emmy award-winning projects. Passionate about functional programming and building high performance real-time systems. Skilled in a variety of programming languages and technologies with expertise in [Rust](https://www.rust-lang.org/) and [TypeScript](https://www.typescriptlang.org/).\n\n---\n\n## Programming Languages\n\n- **Proficient:** Rust, TypeScript (& JavaScript), Elm, Clojure\n- **Experienced:** Elixir, Swift, Objective-C, SQL\n\n---\n\n## Technologies, Skills, and Interests\n\n- **Rust:** Tokio, Futures, Hyper, Actix, Warp\n- **Platforms and Services:** AWS, Docker,  PostgreSQL, Redis, Apache Pulsar, RabbitMQ, Prometheus, GitLab CI/CD, Node.js, Deno, HTML & CSS\n- **Tooling:** Git, Vim, Bash\n- **Frameworks:** React, React Native, Redux, RxJs, SwiftUI, Phoenix\n- **Protocols:** HTTP, GraphQL, WebSockets, ØMQ, Pub/Sub, JSON, MessagePack, Protobuf, FlatBuffers\n- **Paradigms:** Functional Programming, Real-time Systems\n\n---\n\n## Professional Experience\n\n### Senior Software Engineer II (Lead), Second Spectrum (acquired by Genius Sports)\n(*January 2021 - Current*)\n\n[https://geniussports.com/](https://geniussports.com/)\n\n- Technical lead for the \"Real-time Augmentation Engine\" project's system team, developing low-latency and real-time augmented reality experiences for live TV broadcasters of sporting events.\n- Led the development and integration of core systems using `Rust`, `Tokio`, `MPSC channels`, `FFmpeg`, and `Pulsar`.\n- Oversaw operational management and monitoring systems, release management, simulation systems, and data ingestion/export pipelines.\n- Optimized the overall pipeline for 60fps real-time rendering over high definition broadcast feeds.\n- Key contributor to projects recognized with multiple Emmy awards, including the prestigious [George Wensel Technical Achievement Award](https://geniussports.com/customer-stories/cbs-and-genius-win-sports-emmy-for-romovision/) and Amazon's [Outstanding Interactive Experience](https://www.aboutamazon.com/news/entertainment/thursday-night-football-on-prime-video-wins-sports-emmy-award) at the `Sports Emmys`.\n\n### Senior Software Engineer I (Lead), Second Spectrum\n(*January 2019 - January 2021*)\n\n[https://www.secondspectrum.com/](https://www.secondspectrum.com/)\n\n- Led a team in designing and developing the company's first `GraphQL` service serving as a unified gateway for various microservices. Built upon the `Apollo` server framework leveraging `Node.js`, `TypeScript`, `PostgreSQL`, and `Redis` for caching and pub/sub.\n- Developed a dashboard built with `Three.js` for visualizing 3D player tracking datasets and highlighting data discrepancies.\n- Created a prototype `iOS` application for video highlights using `Swift`, `SwiftUI`, and the `Combine` framework.\n- Mentored interns and fellow engineers, fostering collaboration and knowledge sharing.\n\n### Software Engineer, Second Spectrum\n(*August 2016 - January 2019*)\n\n[https://www.secondspectrum.com/](https://www.secondspectrum.com/)\n\n- Developed the operational and quality assurance management application for the company's optical player tracking system. Stack included `Elixir` (with the `Phoenix` framework), `React`, and heavy use of `WebSockets` to sync and display corrections to operators in near real-time.\n- Improved operator efficiency by 100% through the implementation of an automated task selection system.\n- Led the development of the LA Clipper's live streaming web and mobile application \"Clippers CourtVision\". Stack included `Elm` for the web front-end and `React Native` for the mobile app.\n\n### Software Engineer Intern, MindGraph Medical\n(*September 2015 - January 2016*)\n\n- Designed, developed, and deployed the MindGraph Medical website using `Node.js`, `jQuery`, and `Bootstrap`.\n\n---\n\n## Education\n\n**B.S. Computer Science, San Diego State University, San Diego**\n\n(*Class of 2016*)\n\n- [Advanced Certificate in Web and Mobile Applications Development](https://cs.sdsu.edu/about/)\n\n---\n\n## Personal Projects\n\n- [FYGT](https://github.com/DerekCuevas/derekcuevas.github.io): An experimental blog built with `Hugo`, `Deno`, and `ChatGPT`.\n- [Advent of Code 2022](https://github.com/DerekCuevas/advent-of-code-2022): Solutions to Advent of Code programming problems using functional approaches in `Clojure`.\n- [Interview Cake Algorithms Problem Set](https://github.com/DerekCuevas/interview-cake-clj): Solutions to the interview cake programming problems also by exploring functional approaches in `Clojure`.\n- [Friend List](https://github.com/DerekCuevas/friend-list): A non-trivial programming challenge exploring asynchronous side effects with different programming patterns.\n- [Redux Meta Reducer](https://github.com/DerekCuevas/redux-meta-reducer): A `Redux` higher-order reducer for simplifying the state of fetched data.\n- [Avowal](https://github.com/DerekCuevas/avowal): An asynchronous form validation library for `JavaScript`.\n"
  },
  {
   "role": "system",
   "content": "New topics for the blog should be based off of the technical skills, experience, and interests presented in the resume."
  },
  {
   "role": "system",
   "content": "Your previous 40 posts include: \"Building Type-Safe APIs with TypeScript\", \"Implementing the Flyweight Design Pattern in C\", \"Gaining Insights from Large Datasets Using Delta Lake\", \"Using the Observer Design Pattern in Scala\", \"The Builder Design Pattern in TypeScript\", \"Using Array Partitions in Python for Improved Performance\", \"Advanced Techniques for Debugging Rust Programs\", \"Writing a Domain-Specific Language (DSL) in Haskell\", \"Securely Managing Secrets in Production Applications\", \"Implementing the Factory Method Design Pattern in Dart\", \"Boosting Rust Performance with Allocator Selection\", \"Building Real-Time Audio Applications with WebRTC and Rust\", \"Rust's Pinning API: A Comprehensive Guide\", \"Building a Highly Scalable and Fault-Tolerant Load Balancer with Rust\", \"How to Use AWS Fargate for Highly Scalable and Serverless Containers\", \"How to Build a Recommendation Engine with Apache Spark\", \"Building a Distributed Key-Value Store on Top of Apache Kafka\", \"Building a GraphQL API with Actix-Web and Diesel\", \"Building a High-Performance HTTP Proxy with Nginx\", \"How to Use Kafka to Build Scalable and Fault-Tolerant Distributed Systems\", \"Advanced Techniques for Using PostgreSQL JSONB Data Type\", \"Writing Secure Rust Code: A Deep Dive into Memory Safety\", \"Fine-tuning BERT for Question Answering - A Step-by-Step Guide\", \"How to Use the Decorator Pattern for Flexible and Extensible Code\", \"A Deep Dive into Multithreading with Clojure\", \"How to Design Reactive Microservices Using Event-Driven Architecture\", \"Writing Efficient SQL Queries: Tips and Tricks\", \"Mastering Connection Pooling in Django\", \"Mastering the Actor Model Concurrency Paradigm in Elixir\", \"How to use Elm to build maintainable, efficient, and bug-free front-end applications\", \"How to Use Generators to Create Infinite Sequences in Python\", \"Building High Performance Distributed Data Pipelines with Apache Flink\", \"Implementing a Consistent Hashing Algorithm in TypeScript\", \"The Event Sourcing Design Pattern in TypeScript\", \"How to Write Fast Python Code\", \"Building a High-Availability Microservice Architecture with Modern C++\", \"How to Build a Realtime Collaborative Spreadsheet Application with Elixir and Phoenix LiveView\", \"Using the Adapter Pattern to Integrate Legacy Code\", \"Building a Highly Scalable and Performant GraphQL API with Sangria\", \"Building Robust Elixir Back-End APIs with Phoenix Contexts\""
  },
  {
   "role": "system",
   "content": "Your writing style is concise, to the point, informative, and focused on detailed technical information."
  },
  {
   "role": "user",
   "content": "Come up with an advanced topic for an expert level reader that is about a particular single feature, fact, pattern, paradigm, convention, theory, framework, library, or best practice."
  },
  {
   "role": "user",
   "content": "The topic should not be related to a previous post."
  },
  {
   "role": "user",
   "content": "Write a post about the topic following the rules defined."
  },
  {
   "role": "user",
   "content": "Use computer code snippets (in the relevant programming language) in order to illustrate the information presented."
  },
  {
   "role": "user",
   "content": "Format the body of the post in the markdown markup language."
  },
  {
   "role": "user",
   "content": "Include the title of the post on the first line."
  },
  {
   "role": "user",
   "content": "Include one to three tags for the post on the second line formatted as a comma separated list, for example: \"Tags: <tag1>, <tag2>\"."
  },
  {
   "role": "user",
   "content": "Do not include or link to images in the post."
  },
  {
   "role": "user",
   "content": "The post should be multiple sections in length and come to a complete finish."
  }
 ],
 "completion": {
  "id": "chatcmpl-7RP6aPnCAAO0yM6hbJJmwXY7YGk9f",
  "object": "chat.completion",
  "created": 1686765716,
  "model": "gpt-3.5-turbo-0301",
  "usage": {
   "prompt_tokens": 2033,
   "completion_tokens": 1533,
   "total_tokens": 3566
  },
  "choices": [
   {
    "message": {
     "role": "assistant",
     "content": "Title: Building a Highly Scalable Kafka Consumer in Go\n\nTags: Go, Kafka, Distributed Systems\n\n---\n\nApache Kafka is a high-performance distributed streaming platform that can handle large amounts of real-time data. It is widely used for building event-driven systems, real-time processing, and messaging applications. In this article, we will explore how to build a highly scalable Kafka consumer in Go.\n\n## Kafka Consumer Concepts\n\nA Kafka consumer is an application that reads messages (records) from a Kafka topic. Typically, a Kafka consumer is part of a larger distributed system that processes and analyzes data in real-time. \n\nWhen building a Kafka consumer, there are a few important concepts to keep in mind:\n\n- **Topic** - A Kafka topic is a category or feed name that messages are published to and consumed from. A topic can have one or more partitions.\n- **Partition** - A topic is divided into one or more partitions, which allow multiple consumers to read from a topic in parallel. Each partition is an ordered and immutable sequence of records.\n- **Offset** - An offset is a unique identifier that represents a specific location in a partition.\n- **Consumer Groups** - A consumer group is a set of consumers that work together to consume messages from a topic. Each consumer in a group reads from a unique partition. The Kafka broker automatically balances partition assignments across consumer instances in a group.\n\n## Setting Up a Kafka Consumer in Go\n\nTo get started with building a Kafka consumer in Go, we need a few dependencies. First, we need the `sarama` Kafka client. This client library is officially supported by the Apache Kafka community and provides full support for Kafka operations, including topics, messages, producers, and consumers.\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/Shopify/sarama\"\n)\n```\n\nNext, we need to create a new Kafka consumer instance using the `sarama` client library.\n\n```go\nfunc main() {\n    config := sarama.NewConfig()\n    config.Consumer.Return.Errors = true\n\n    brokers := []string{\"localhost:9092\"}\n    topics := []string{\"my-topic\"}\n\n    consumer, err := sarama.NewConsumer(brokers, config)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer func() {\n        if err := consumer.Close(); err != nil {\n           log.Fatal(err)\n        }\n    }()\n}\n```\n\nWe can now create a Kafka consumer group member and join a consumer group. When joining a consumer group, we also need to provide a name for the group. Consumer group names help identify different consumer groups in the system and enable Kafka brokers to balance partition assignments accordingly.\n\n```go\n    group, err := sarama.NewConsumerGroup(brokers, groupID, config)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer func() {\n        if err := group.Close(); err != nil {\n            log.Fatal(err)\n        }\n    }()\n\n    handler := ConsumerHandler{}\n\n    for {\n        err := group.Consume(context.Background(), topics, &handler)\n        if err != nil {\n            log.Fatal(err)\n        }\n    }\n```\n\nFinally, we need to define a Kafka message handler function that gets called every time a new message is received by the consumer. \n\n```go\ntype ConsumerHandler struct {\n}\n\nfunc (c *ConsumerHandler) Setup(_ sarama.ConsumerGroupSession) error {\n    return nil\n}\n\nfunc (c *ConsumerHandler) Cleanup(_ sarama.ConsumerGroupSession) error {\n    return nil\n}\n\nfunc (c *ConsumerHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n    for msg := range claim.Messages() {\n        fmt.Printf(\"Message Topic:%q Partition:%d Offset:%d Value:%s\\n\", msg.Topic, msg.Partition, msg.Offset, string(msg.Value))\n        session.MarkMessage(msg, \"\")\n    }\n    return nil\n}\n```\n\nThe `ConsumeClaim` function is where we will define our message processing logic. For each message, we will print the message's topic, partition, offset, and value and then mark the message as processed using the `MarkMessage` function.\n\nWith our Kafka consumer group member created, we can now start processing messages from our Kafka topic.\n\n## Improving Kafka Consumer Performance\n\nIn a real-world distributed system, it is essential to build a Kafka consumer that can handle high message volumes without compromising performance and scalability.\n\nOne technique for improving Kafka consumer performance is to use a worker pool to distribute message processing across multiple worker threads. By doing so, we can improve message throughput and keep our Kafka consumers responsive even under high message volumes.\n\n```go\nfunc main() {\n    // ...\n\n    workerCount := 10\n\n    group, err := sarama.NewConsumerGroup(brokers, groupID, config)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer func() {\n        if err := group.Close(); err != nil {\n            log.Fatal(err)\n        }\n    }()\n\n    for i := 0; i < workerCount; i++ {\n        go func() {\n            handler := ConsumerHandler{}\n\n            for {\n                err := group.Consume(context.Background(), topics, &handler)\n                if err != nil {\n                    log.Printf(\"Error processing message: %v\", err)\n                }\n            }\n        }()\n    }\n\n    // ...\n}\n```\n\nBy creating a worker pool with multiple worker threads, we can increase message throughput and improve resilience to high message volumes.\n\nAnother technique to improve Kafka consumer performance is to pre-fetch messages from Kafka using the `Prefetch` setting. Pre-fetching messages allows the Kafka consumer to buffer unread messages in memory, reducing the number of round trips between the Kafka broker and consumer and improving throughput.\n\n```go\n    config := sarama.NewConfig()\n    config.Consumer.Return.Errors = true\n    config.Consumer.MaxWaitTime = 1 * time.Second\n    config.Consumer.Fetch.Default = 1024 * 1024\n    config.Consumer.Fetch.Min = 128 * 1024\n    config.Consumer.Fetch.Max = 10 * 1024 * 1024\n    config.Consumer.Group.Session.Timeout = 30 * time.Second\n    config.Consumer.Group.Heartbeat.Interval = 10 * time.Second\n    config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRoundRobin\n    config.Consumer.Group.Rebalance.Timeout = 30 * time.Second\n    config.Consumer.Group.Rebalance.Retry.Max = 2\n\n    config.ChannelBufferSize = 256\n    config.Version = sarama.MaxVersion\n    config.Metadata.Full = false\n    config.Metadata.Retry.Max = 3\n\n    config.Consumer.MaxProcessingTime = 250 * time.Millisecond\n    config.Consumer.Prefetch = 500\n\n    // ...\n```\n\nBy setting the `Prefetch` configuration option, we can control the maximum number of messages we want to buffer in memory, which improves message throughput.\n\n## Conclusion\n\nIn this article, we explored how to build a highly scalable Kafka consumer in Go. We covered the basics of Kafka consumer concepts and saw how to use the `sarama` client library to create and configure a Kafka consumer instance.\n\nWe then looked at some techniques to improve Kafka consumer performance, including using a worker pool and pre-fetching messages. By implementing these techniques, we can build performant and scalable Kafka consumers that can handle high message volumes without compromising performance."
    },
    "finish_reason": "stop",
    "index": 0
   }
  ]
 }
}