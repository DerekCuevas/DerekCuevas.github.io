{
 "prompt": [
  {
   "role": "system",
   "content": "You are the author of a personal technical blog about software engineering and computer science subjects."
  },
  {
   "role": "system",
   "content": "Write as though you are the author of this resume:\n+++\ntitle = \"Résumé\"\ndescription = \"Derek Cuevas's Résumé\"\ndate = \"2023-05-20\"\nauthor = \"Derek Cuevas\"\n+++\n\nSenior Software Engineering Tech Lead with 8 years of professional experience building full stack applications and highly available, performance critical, realtime systems and services.\n\n## Programming Languages\n\n- Rust\n- TypeScript\n- Elm\n- Swift\n- Elixir\n- Clojure\n- SQL\n- HTML & CSS\n\n## Technologies, Skills, and Interests\n\n- **Paradigms:** Functional Programming, Realtime Systems, Distributed Systems, Data Structures, Algorithms\n- **Rust:** Futures, Async/Await, Tokio, Hyper, Actix, MPSC Channels, Serde\n- **Protocols:** HTTP, GraphQL, WebSockets, TCP, HLS, DASH, ZMQ, Flatbuffers, Protobuf, MsgPack, JSON\n- **Services:** Node.js, AWS (S3, EC2, SQS, ect.), PostgreSQL, Redis, Pulsar, RabbitMQ\n- **Frameworks:** React, React Native, Redux, RxJs, SwiftUI\n- **Tooling:** Git, Vim, Docker, Bash\n"
  },
  {
   "role": "system",
   "content": "New topics for the blog should be based off of the technical skills, experience, and interests presented in the resume."
  },
  {
   "role": "system",
   "content": "Your previous 20 posts include: \"Building a GraphQL API with Actix-Web and Diesel\", \"Building a High-Performance HTTP Proxy with Nginx\", \"How to Use Kafka to Build Scalable and Fault-Tolerant Distributed Systems\", \"Advanced Techniques for Using PostgreSQL JSONB Data Type\", \"Writing Secure Rust Code: A Deep Dive into Memory Safety\", \"Fine-tuning BERT for Question Answering - A Step-by-Step Guide\", \"How to Use the Decorator Pattern for Flexible and Extensible Code\", \"A Deep Dive into Multithreading with Clojure\", \"How to Design Reactive Microservices Using Event-Driven Architecture\", \"Writing Efficient SQL Queries: Tips and Tricks\", \"Mastering Connection Pooling in Django\", \"Mastering the Actor Model Concurrency Paradigm in Elixir\", \"How to use Elm to build maintainable, efficient, and bug-free front-end applications\", \"How to Use Generators to Create Infinite Sequences in Python\", \"Building High Performance Distributed Data Pipelines with Apache Flink\", \"Implementing a Consistent Hashing Algorithm in TypeScript\", \"The Event Sourcing Design Pattern in TypeScript\", \"How to Write Fast Python Code\", \"Building a High-Availability Microservice Architecture with Modern C++\", \"How to Build a Realtime Collaborative Spreadsheet Application with Elixir and Phoenix LiveView\""
  },
  {
   "role": "system",
   "content": "Your writing style is concise, to the point, informative, and focused on detailed technical information."
  },
  {
   "role": "user",
   "content": "Come up with an advanced topic for an expert level reader that is about a particular single feature, fact, pattern, paradigm, convention, theory, framework, library, practical example, or best practice."
  },
  {
   "role": "user",
   "content": "The topic should not be related to a previous post."
  },
  {
   "role": "user",
   "content": "Write a post about the topic following the rules defined."
  },
  {
   "role": "user",
   "content": "Use computer code snippets (in the relevant programming language) in order to illustrate the information presented."
  },
  {
   "role": "user",
   "content": "Format the body of the post in the markdown markup language."
  },
  {
   "role": "user",
   "content": "Include the title of the post on the first line."
  },
  {
   "role": "user",
   "content": "Include one to three tags for the post on the second line formatted as a comma separated list, for example: \"Tags: <tag1>, <tag2>\"."
  },
  {
   "role": "user",
   "content": "Do not include images in the post."
  },
  {
   "role": "user",
   "content": "The post should be multiple sections in length and come to a complete finish."
  }
 ],
 "completion": {
  "id": "chatcmpl-7PxCHjnlCMg9tWK2vswTdtHNlRhr7",
  "object": "chat.completion",
  "created": 1686420109,
  "model": "gpt-3.5-turbo-0301",
  "usage": {
   "prompt_tokens": 830,
   "completion_tokens": 1027,
   "total_tokens": 1857
  },
  "choices": [
   {
    "message": {
     "role": "assistant",
     "content": "Building a Distributed Key-Value Store on Top of Apache Kafka\n\nTags: Apache Kafka, Distributed Systems, Data Storage\n\n## Introduction\n\nIn a typical distributed system, there are many challenges that must be addressed when it comes to storing and retrieving data. One common approach to this problem is the use of a key-value store. A key-value store is a data storage system that allows you to store and retrieve data based on a unique key.\n\nIn this post, we'll explore how to build a key-value store on top of Apache Kafka, a distributed stream processing system. We'll discuss how to use Kafka to store and retrieve key-value data, and provide a few code snippets along the way.\n\n## Apache Kafka\n\nApache Kafka is a distributed stream processing system that is commonly used for building high-throughput, fault-tolerant, and scalable applications. At its core, Kafka is a distributed log system that allows you to store records in partitions across multiple nodes. \n\nIn Kafka, partitions are the unit of parallelism. Each partition is an ordered, immutable sequence of records that can be consumed by one or many consumers.\n\n## Getting Started\n\nTo get started, we'll need to create a new Kafka topic to store our key-value data. We'll create a topic with three partitions, a replication factor of three, and a cleanup policy of `compact`, which is optimized for storing key-value data.\n\n```bash\n$ bin/kafka-topics.sh --create \\\n    --bootstrap-server localhost:9092 \\\n    --replication-factor 3 \\\n    --partitions 3 \\\n    --topic my-key-value-store \\\n    --config cleanup.policy=compact\n```\n\nNext, we'll write some code to produce values to our new Kafka topic. For the sake of simplicity, we'll assume that our key-value store only holds strings.\n\n```java\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport java.util.Properties;\n\npublic class MyValueProducer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(\"bootstrap.servers\", \"localhost:9092\");\n        props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        \n        KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n        \n        String key = \"my-key\";\n        String value = \"Hello, world!\";\n        \n        ProducerRecord<String, String> record = new ProducerRecord<>(\"my-key-value-store\", key, value);\n        \n        producer.send(record);\n        producer.close();\n    }\n}\n```\n\nThis code creates a new Kafka producer, sends a single key-value record to our topic, and then closes the producer.\n\n## Retrieving Values\n\nNow that we've written some key-value data to our Kafka topic, let's look at how we can retrieve it. We'll use the Kafka Streams API to create a new stream processor that reads data from our topic, and prints out each key-value pair that it reads.\n\n```java\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.kstream.KStream;\n\nimport java.util.Properties;\n\npublic class MyValueProcessor {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"my-key-value-processor\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        \n        final Serde<String> stringSerde = Serdes.String();\n        final Serde<byte[]> dataSerde = Serdes.ByteArray();\n        \n        StreamsBuilder builder = new StreamsBuilder();\n        \n        KStream<String, String> myKeyValueStream = builder.stream(\"my-key-value-store\");\n        \n        myKeyValueStream.foreach((key, value) -> {\n            System.out.printf(\"key = %s, value = %s\\n\", key, value);\n        });\n        \n        KafkaStreams stream = new KafkaStreams(builder.build(), props);\n        stream.start();\n    }\n}\n```\n\nThis code creates a new Kafka Streams application, which reads data from our key-value store and uses a `foreach` method to print out each key-value pair.\n\n## Conclusion\n\nIn this post, we've seen how to build a distributed key-value store on top of Apache Kafka using the Kafka Streams API. We've explored how to create a Kafka topic optimized for storing key-value data, and seen how to use the Kafka producer API to write values to our topic. We've also seen how to use the Kafka Streams API to read values from our topic.\n\nAlthough the example above is for strings only, it can be extended to other data types as well, such as JSON or Avro. The possibilities are endless with Kafka and the Kafka Streams API."
    },
    "finish_reason": "stop",
    "index": 0
   }
  ]
 }
}